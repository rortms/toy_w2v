{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import itertools as itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Infinite data iterator\n",
    "def iterData(filename, batch_size, jump_around=False):\n",
    "\n",
    "    # Iterator reading file in batches\n",
    "    def get_batch():\n",
    "    \n",
    "        with open(filename, \"r\") as datfile:\n",
    "\n",
    "            position, offset = 0, batch_size\n",
    "\n",
    "            while True:\n",
    "                \n",
    "                if jump_around:\n",
    "                    offset = np.random.randint(1,4) * batch_size // 3\n",
    "                    \n",
    "                yield datfile.read(batch_size)\n",
    "                position = datfile.seek(position + offset)\n",
    "\n",
    "    batch = get_batch()\n",
    "\n",
    "    # When file is exhausted, start over with new get_batch iterator\n",
    "    while True:\n",
    "        b = next(batch)\n",
    "        if b:\n",
    "            yield  b.split()\n",
    "        else:\n",
    "            print(\"EOF, starting over\")\n",
    "            batch = get_batch()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "['Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly']\n"
     ]
    }
   ],
   "source": [
    "# Test infinite iterator\n",
    "fn = \"./testf\"\n",
    "\n",
    "with open(fn, \"r\") as tf:\n",
    "    lines = tf.read()\n",
    "print(len(lines))\n",
    "\n",
    "# Only 54 charactors reused indefinitely\n",
    "testIter = iterData(fn, 5)\n",
    "print(list( (\" \".join(next(testIter)) for k in range(100)) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Initialize file iterator\n",
    "from config import settings\n",
    "\n",
    "moreData = iterData(settings[\"data_path\"], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220831\n"
     ]
    }
   ],
   "source": [
    "# Construct vocabulary set\n",
    "all_words = Counter(next(moreData))\n",
    "print(len(all_words))\n",
    "for k in range(70000):\n",
    "    all_words.update(next(moreData))\n",
    "print(len(all_words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "words = Counter( { w[0] : w[1] for w in all_words.most_common(int(80e3)) } )\n",
    "words['UNKN'] = 0\n",
    "for w in all_words:\n",
    "    if words.get(w, None) is None:\n",
    "        words['UNKN'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80001\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39075\n"
     ]
    }
   ],
   "source": [
    "# Subsampling with word2vec's subsampling function\n",
    "# following: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "\n",
    "# Increase the chance of a less common word being kept, and reduce that of very common words\n",
    "def P(word_fraction):\n",
    "\n",
    "    return np.sqrt(1e-7/word_fraction) + 1e-7/word_fraction\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "m = min(words.values())\n",
    "\n",
    "total_words = sum(words.values())\n",
    "\n",
    "## 0.75 power of fraction taken from word 2 vec\n",
    "words = Counter({ w : int( (100*count)**0.75 )  for w, count in words.most_common() if np.random.uniform() >= P(count/total_words)})\n",
    "\n",
    "\n",
    "# Word to id mappings\n",
    "word2int = { tup[0] : i for i, tup in enumerate(words.most_common()) }\n",
    "int2word = { i : word for word, i in word2int.items() }\n",
    "\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Skip-gram model: for each word, sample a surrounding word within a fixed window (skip-window) excluding itself,\n",
    "# each word is processed in this way k times (skip number) generating k training targets for it.\n",
    "#\n",
    "\n",
    "def makeInputsTargets(word_sequence, radius = 4, repeat_num = 2):\n",
    "\n",
    "    words = np.zeros(len(word_sequence)*repeat_num, dtype=np.int32)\n",
    "    targets =  np.zeros((len)(word_sequence)*repeat_num, dtype=np.int32 )\n",
    "    batch_size = len(words) # or targets\n",
    "    \n",
    "    for i in range(0, batch_size, repeat_num):\n",
    "        \n",
    "        # Index in word_sequence array ( len(word_sequence) < batch_size )\n",
    "        index = i // repeat_num\n",
    "        word = word_sequence[index]\n",
    "        \n",
    "        lower = 0 if index < radius else index - radius\n",
    "        upper = index + radius\n",
    "        words_in_window = word_sequence[lower:index] + word_sequence[index+1:upper]\n",
    "        \n",
    "        for k in range(repeat_num):\n",
    "            words[i+k] = word\n",
    "            targets[i+k] = words_in_window[np.random.randint(0,len(words_in_window))]\n",
    "\n",
    "    return words, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class littleNN(object):\n",
    "\n",
    "    def __init__(self, word_id_counts, embedding_dim, neg_sample_size, learning_rate = 0.5):\n",
    "\n",
    "        ## Vocabulary\n",
    "        self.vocab_size = len(word_id_counts)\n",
    "        self.unigram_table = list(word_id_counts.elements()) # immitating original w2v\n",
    "        \n",
    "        ## Network parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        \n",
    "        # layers\n",
    "        self.w0 = np.random.normal(0, self.vocab_size**-0.5, (self.vocab_size, embedding_dim ) )\n",
    "        self.b0 = np.zeros(embedding_dim)\n",
    "        # For softmax implementation\n",
    "        self.w1 = np.random.normal(0, self.vocab_size**-0.5, (embedding_dim, self.vocab_size) )\n",
    "        self.b1 = np.zeros(self.vocab_size)\n",
    "        # For negative sampling implementation\n",
    "        self.u = np.random.normal(0, self.vocab_size**-0.5, (embedding_dim, 1) )\n",
    "        self.b = np.random.normal(0, self.vocab_size**-0.5)\n",
    "        \n",
    "        # Hyperparams\n",
    "        self.lr = learning_rate\n",
    "        self.reg = 1e-3\n",
    "\n",
    "        # Activations\n",
    "        self.sgmd = lambda x: 1 / (1 + np.exp(-x)) # Sigmoid\n",
    "\n",
    "        \n",
    "    # Softmax\n",
    "    def Softmax(self, v):\n",
    "\n",
    "        # Numerical stability, (avoiding large number overflow)\n",
    "        C = np.max(v) / 4\n",
    "\n",
    "        expV = np.exp(v-C)\n",
    "        return expV / np.sum(expV, axis=1, keepdims=True)\n",
    "    \n",
    "    # Negative sampling\n",
    "    def negSample(self, target_id):\n",
    "\n",
    "        target_id = target_id[0]\n",
    "        neg_samples = np.zeros(self.neg_sample_size+1, dtype=np.int32)\n",
    "        for k in range(self.neg_sample_size):\n",
    "\n",
    "            # Imitating unigram table idea from original w2v, see;\n",
    "            # http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "            \n",
    "            # neg_word_id = self.unigram_table[np.random.randint(len(self.unigram_table))]\n",
    "            # neg_samples[neg_word_id] = 0 # one-hot encoding to 0\n",
    "            samp = self.unigram_table[np.random.randint(len(self.unigram_table))]\n",
    "            if samp != target_id:\n",
    "                neg_samples[k] = samp\n",
    "            else:\n",
    "                neg_samples[k] = self.unigram_table[np.random.randint(len(self.unigram_table))]\n",
    "\n",
    "        neg_samples[-1] = target_id\n",
    "        return neg_samples\n",
    "    \n",
    "    #\n",
    "    def forwardPass(self, Xbatch):\n",
    "\n",
    "        # Input to hidden is just a lookup (b/c of one hot word encoding)\n",
    "        # z0 = np.array( [self.w0[word_id] + self.b0 for word_id in inBatch] )\n",
    "        z0 = self.w0[Xbatch] + self.b0\n",
    "        a0 = np.maximum(0, z0) # ReLu activation\n",
    "\n",
    "        # Hidden to output\n",
    "        z1 = a0 @ self.w1 + self.b1\n",
    "        return a0, z1\n",
    "    \n",
    "    #\n",
    "    def sampledBackProp(self, a0, z1, Xbatch, target_batch):\n",
    "\n",
    "        N = len(Xbatch)\n",
    "        \n",
    "        # Last element contains target/positive index (see negSample above). All others considered negative\n",
    "        sample_indices = np.apply_along_axis(self.negSample, 1, target_batch[:,None]) # See explanatory ex1 below\n",
    "\n",
    "        z1 = z1[np.arange(N)[:,None], sample_indices] # See explanatory ex2 below\n",
    "        out = self.sgmd(z1)\n",
    "\n",
    "        n = z1.shape[1] # \"binary\" classification occurs along sample size, not batch size\n",
    "        # Sigmoid error\n",
    "        out_d = -out / n\n",
    "        out_d[:, -1] += 1 / n  # i.e err = (y - sgmd(z1)) where y = 0,1\n",
    "        \n",
    "        # Create sparse csr matrix\n",
    "        rows = np.array([ [ k for _ in range(len(sample_indices[k])) ] for k in range(N)]).flatten()\n",
    "        columns = sample_indices.flatten()\n",
    "        data = out_d.flatten()\n",
    "        sparse_out_d = csr_matrix( (data, (rows, columns)), shape=(N, self.vocab_size))\n",
    "                \n",
    "        ## w1 ##\n",
    "        \n",
    "        w1_D = a0.T @ sparse_out_d\n",
    "        b1_d = np.sum(out_d, axis=0)\n",
    "        \n",
    "        w1_D += self.reg * self.w1 # regularization gradient\n",
    "        \n",
    "        # update\n",
    "        self.w1 += -self.lr * w1_D\n",
    "        self.b1[sample_indices] += -self.lr * b1_d\n",
    "\n",
    "        \n",
    "        ## w0 ##\n",
    "\n",
    "        # Local gradient of ReLu a0 is 1 if z0>0 else 0\n",
    "        a0_d = sparse_out_d @ self.w1.T\n",
    "        a0_d[a0 <= 0] = 0\n",
    "        \n",
    "        b0_d = np.sum(a0_d, axis=0)\n",
    "\n",
    "        # Xbatch as sparse matrix\n",
    "        rows = range(N)\n",
    "        columns = Xbatch\n",
    "        data = np.ones(N)\n",
    "        \n",
    "        sparse_Xbatch = csr_matrix( (data, (rows, columns)), shape=(N, self.vocab_size))\n",
    "        w0_D = sparse_Xbatch.T @ a0_d\n",
    "        \n",
    "        w0_D += self.reg * self.w0 # regularization gradient\n",
    "        \n",
    "        # update\n",
    "        self.w0 += -self.lr * w0_D\n",
    "        self.b0 += -self.lr * b0_d\n",
    "    \n",
    "    #\n",
    "    def backpropUpdate(self, a0, z1, Xbatch, target_batch):\n",
    "        \n",
    "        softmax_p = self.Softmax(z1)\n",
    "        N = softmax_p.shape[0]\n",
    "        softmax_p_d = softmax_p\n",
    "        softmax_p_d[range(N), target_batch] -= 1\n",
    "        softmax_p_d /= N # weight updates will be averaged over batch size\n",
    "\n",
    "        ## w1\n",
    "        w1_D = np.dot(a0.T, softmax_p_d)\n",
    "        b1_d = np.sum(softmax_p_d, axis=0)\n",
    "\n",
    "        w1_D += self.reg * self.w1 # regularization gradient\n",
    "        \n",
    "        # update\n",
    "        self.w1 += -self.lr * w1_D\n",
    "        self.b1 += -self.lr * b1_d\n",
    "\n",
    "        ## w0\n",
    "\n",
    "        # Local gradient of ReLu a0 is 1 if z0>0 else 0\n",
    "        a0_d = np.dot(softmax_p_d, self.w1.T)\n",
    "        a0_d[ a0_d <= 0 ] = 0 \n",
    "        b0_d = np.sum(a0_d, axis=0)\n",
    "        \n",
    "        # Treating Xbatch as one-hot implies w1_D consists of copies of\n",
    "        # a0_d as rows, for every xi = 1, and a row of zero otherwise\n",
    "        \n",
    "        # one-hot equivalent of np.dot(Xbatch.T, a0_d) combined with\n",
    "        # update step.\n",
    "\n",
    "        for k, index in enumerate(Xbatch):\n",
    "            self.w0[index] += -self.lr * a0_d[k]\n",
    "            \n",
    "        self.b0 += -self.lr * b0_d\n",
    "    \n",
    "    #\n",
    "    def crossEntropy(self, out, target_ids):\n",
    "\n",
    "        N = out.shape[0]\n",
    "        return -np.log(out[range(N), target_ids]).sum() / N\n",
    "    \n",
    "    #\n",
    "    def word2vec(self, in_word_id):\n",
    "        return self.w0[in_word_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 4],\n       [0, 0, 2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,4],[1,1,1]]) *  np.array([[1,0,1],[0,0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8., 8., 8., 8., 0., 0.],\n       [8., 0., 0., 0., 0., 0.],\n       [0., 8., 0., 8., 8., 0.],\n       [8., 8., 0., 8., 8., 8.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(2*np.random.randint(0,2, (6,4)).T) @ (4*np.eye(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets [15  7  1  8 17  6 18 13  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7,  3, 11,  4],\n       [15,  9, 16, 10],\n       [ 7,  8, 13,  2],\n       [16, 17, 11, 13],\n       [14,  6, 12,  4],\n       [14,  8,  9, 16],\n       [ 6,  2, 13, 16],\n       [ 2,  6, 10,  7],\n       [15, 10,  8, 17],\n       [ 9, 10, 19, 13]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "# Explanatory Ex 1\n",
    "tars = np.random.randint(0,20, 10)\n",
    "                \n",
    "print(\"targets\", tars)\n",
    "                \n",
    "def getNegSample(target, N=20):\n",
    "\n",
    "    target = target[0]\n",
    "    \n",
    "    if target < (N-1) / 2:\n",
    "\n",
    "        return random.sample(range(target+1, N), 4)\n",
    "\n",
    "    return random.sample(range(0, target-1), 4)\n",
    "\n",
    "# for each target apply getNegSample function\n",
    "np.apply_along_axis(getNegSample, 1, tars[:,None])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]\n",
      " [11 10  9  8  7  6  5  4]]\n",
      "[[5 0]\n",
      " [5 0]\n",
      " [5 0]\n",
      " [5 0]\n",
      " [5 0]\n",
      " [5 0]\n",
      " [5 0]]\n",
      "[[ 6 11]\n",
      " [ 6 11]\n",
      " [ 6 11]\n",
      " [ 6 11]\n",
      " [ 6 11]\n",
      " [ 6 11]\n",
      " [ 6 11]]\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Explanatory Ex 2\n",
    "\n",
    "l = 7\n",
    "ints = np.array([range(11, 3, -1) for k in range(l)])\n",
    "indices = np.array([[5,0] for k in range(l)])\n",
    "print(ints)\n",
    "print(indices)\n",
    "select_ints = ints[np.arange(l)[:,None],indices]\n",
    "\n",
    "print(select_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows [0 0 1 1 2 2 3 3 4 4 5 5 6 6]\n",
      "columns [5 0 5 0 5 0 5 0 5 0 5 0 5 0]\n",
      "data [ 6 11  6 11  6 11  6 11  6 11  6 11  6 11]\n",
      "[[11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]\n",
      " [11  0  0  0  0  6  0  0]]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Explanatory Ex 3\n",
    "\n",
    "rows = np.array([ [k for l in range(len(indices[k]))] for k in range(l) ]).flatten()\n",
    "print(\"rows\", rows)\n",
    "columns = indices.flatten()\n",
    "print(\"columns\", columns)\n",
    "data = select_ints.flatten()\n",
    "\n",
    "print(\"data\", data)\n",
    "\n",
    "csr = csr_matrix( (data, (rows, columns)), shape=(7,8))\n",
    "print(csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dot in module scipy.sparse.base:\n",
      "\n",
      "dot(self, other)\n",
      "    Ordinary dot product\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from scipy.sparse import csr_matrix\n",
      "    >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
      "    >>> v = np.array([1, 0, -1])\n",
      "    >>> A.dot(v)\n",
      "    array([ 1, -3, -1], dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(csr_matrix.dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cosineDistance(v,w):\n",
    "\n",
    "    unit = lambda x: x / np.sqrt(np.dot(x,x))\n",
    "    if (v**2).sum() < 1e-7:\n",
    "        raise Exception('First argument to cosineDistance is close to zero vector')\n",
    "    if (w**2).sum() < 1e-7:\n",
    "        raise Exception('Second argument to cosineDistance is close to zero vector')    \n",
    "    return np.dot( unit(v), unit(w) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Training Params\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "# Hyperparams\n",
    "embedding_dim = 128\n",
    "neg_sample_size = 64 #300\n",
    "learning_rate = 0.05\n",
    "# momentum = 0.5\n",
    "\n",
    "# word2vec and validation\n",
    "radius = 4#2       # sample window radius\n",
    "repeat_num = 3#2   # number of times to sample each word\n",
    "head_subset_size = 100 # sample only from head of distribution for monitoring progress\n",
    "validation_sample_size = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to history: ['paying', 'panic', 'mediaeval', 'czech', 'westerners', 'miniszt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to were: ['members', 'erfurt', 'nucleophilic', 'hegelian', 'bullfighting', 'quite']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to UNKN: ['coals', 'dubbed', 'px', 'extrapolating', 'banana', 'barnes']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to also: ['terraforming', 'atmospheres', 'pigs', 'prusias', 'ergaster', 'allegory']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to more: ['housing', 'adige', 'creams', 'fiba', 'vehicular', 'aleman']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to a: ['youths', 'rhyme', 'capers', 'cursing', 'marc', 'convention']\n",
      "------------\n",
      "\n",
      "Loss:  10.57316260289709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  10.380614960639381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  10.13609994294196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  10.056556099888773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  9.397213355251855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  13.849286359178146\n",
      "Seems to be diverging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to history: ['paying', 'panic', 'mediaeval', 'czech', 'miniszt', 'westerners']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to were: ['the', 'of', 'a', 'in', 'one', 'decades']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to UNKN: ['in', 'the', 'of', 'and', 'a', 'coloured']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to also: ['terraforming', 'atmospheres', 'pigs', 'prusias', 'ergaster', 'allegory']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to more: ['housing', 'adige', 'creams', 'appian', 'fiba', 'vehicular']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to a: ['in', 'the', 'of', 'and', 'a', 'for']\n",
      "------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to history: ['paying', 'panic', 'mediaeval', 'czech', 'miniszt', 'westerners']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to were: ['the', 'of', 'a', 'in', 'one', 'decades']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to UNKN: ['in', 'the', 'of', 'and', 'a', 'coloured']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to also: ['terraforming', 'atmospheres', 'pigs', 'prusias', 'ergaster', 'allegory']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to more: ['housing', 'adige', 'creams', 'appian', 'fiba', 'vehicular']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words close to a: ['in', 'the', 'of', 'and', 'a', 'for']\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init NN\n",
    "word_id_counts = Counter( { word2int[w] : words[w] for w in words } )\n",
    "\n",
    "net = littleNN(word_id_counts, embedding_dim, neg_sample_size, learning_rate)\n",
    "\n",
    "def trainNN(word_id_counts):\n",
    "\n",
    "    \n",
    "    # Data iterator\n",
    "    moreData = iterData(settings[\"data_path\"], batch_size, jump_around=True) # data iterator\n",
    "    \n",
    "\n",
    "    ###############\n",
    "    # Random sample from dictionary to observe closest cosine distances\n",
    "    head_of_distribution = [ id_count[0] for id_count in word_id_counts.most_common( head_subset_size )]\n",
    "    samp_ids = random.sample(head_of_distribution, validation_sample_size)\n",
    "    pS = pd.Series([w_id for w_id in word_id_counts.keys() if w_id not in samp_ids])\n",
    "\n",
    "    def report_closest():\n",
    "        for samp_id in samp_ids:\n",
    "            neighbor_ids = pS.apply( lambda w_id: cosineDistance(net.word2vec(samp_id), net.word2vec(w_id)) ).nlargest(6).index.values\n",
    "            print(\"Words close to {}: {}\".format(int2word[samp_id], [int2word[i] for i in neighbor_ids] ) )\n",
    "        print(\"------------\\n\")        \n",
    "    ##############\n",
    "    #\n",
    "    prev_loss = 1000\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # for v in [net.word2vec(i) for i in samp_ids]:\n",
    "        #     print(v)\n",
    "        # print(\"-------------------\\n\")\n",
    "        \n",
    "        \n",
    "        ## View sampled words' neighbors every epoch\n",
    "        if e % 200 == 0:\n",
    "            report_closest()\n",
    "        \n",
    "        ## Train ##\n",
    "        \n",
    "        batch = next(moreData)\n",
    "        batch = [ word2int[word] if word2int.get(word,None) is not None else word2int['UNKN'] for word in batch  ]\n",
    "        \n",
    "        word_id_batch, target_batch = makeInputsTargets(batch, radius=radius, repeat_num=repeat_num )\n",
    "\n",
    "        a0, z1 = net.forwardPass(word_id_batch)\n",
    "\n",
    "        # Report loss Early break tweak learning rate\n",
    "        if e % 20 == 0:\n",
    "            loss = net.crossEntropy(net.Softmax(z1), target_batch)\n",
    "            print(\"Loss: \", loss)\n",
    "            if loss - prev_loss > 2:\n",
    "                print(\"Seems to be diverging\")\n",
    "                report_closest()\n",
    "                break\n",
    "            if loss - prev_loss > 0.1:\n",
    "                net.lr *= 0.1\n",
    "            prev_loss = loss\n",
    "            \n",
    "        \n",
    "        net.sampledBackProp(a0, z1, word_id_batch, target_batch)\n",
    "        \n",
    "    report_closest()\n",
    "    \n",
    "## Run training\n",
    "\n",
    "trainNN(word_id_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/krolik/anaconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.380726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.065016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.291657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.786741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.383429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.089657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.94431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.516418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.332522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0436125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.674659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.97535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.056026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.718421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.905842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4381485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.216802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.956293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.497804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.594325\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Params\n",
    "epochs = 1000\n",
    "batch_size = 1000\n",
    "# Hyperparams\n",
    "embedding_dim = 128\n",
    "neg_sample_size = 64 #300\n",
    "learning_rate = 0.05\n",
    "\n",
    "# word2vec and validation\n",
    "radius = 6#2       # sample window radius\n",
    "repeat_num = 3#2   # number of times to sample each word\n",
    "head_subset_size = 100 # sample only from head of distribution for monitoring progress\n",
    "validation_sample_size = 6\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "moreData = iterData(settings[\"data_path\"], batch_size)\n",
    "\n",
    "# Input\n",
    "word_indices = tf.placeholder(tf.int32, [None])\n",
    "labels = tf.placeholder(tf.int32, [None, None])\n",
    "\n",
    "# Embedding\n",
    "embedding_layer = tf.get_variable(\"Embedding_Layer\",\n",
    "                                  initializer=tf.truncated_normal([len(words), embedding_dim]))\n",
    "\n",
    "embed_vecs = tf.nn.embedding_lookup(embedding_layer, word_indices)\n",
    "\n",
    "softmax_out = tf.get_variable(\"Softmax_Out\", \n",
    "                              initializer=tf.truncated_normal([len(words), embedding_dim])) # Order reversed from expected\n",
    "\n",
    "biases_out = tf.get_variable(\"Biases_Out\",\n",
    "                             initializer=tf.zeros(len(words)))\n",
    "\n",
    "#\n",
    "loss = tf.reduce_mean ( tf.nn.sampled_softmax_loss( softmax_out,\n",
    "                                                    biases_out,\n",
    "                                                    labels,\n",
    "                                                    embed_vecs,\n",
    "                                                    neg_sample_size,\n",
    "                                                    len(words) ) )\n",
    "#\n",
    "optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n",
    "\n",
    "with tf.Session() as sesh:\n",
    "    \n",
    "    sesh.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(10000):\n",
    "        batch = next(moreData)\n",
    "        batch = [ word2int[word] if word2int.get(word,None) is not None else word2int['UNKN'] for word in batch  ]        \n",
    "        word_ids, label_ids = makeInputsTargets(batch, radius, repeat_num)\n",
    "\n",
    "        d = { word_indices : word_ids, labels : np.array(label_ids)[:,None] }\n",
    "        \n",
    "        _, l = sesh.run([optimizer, loss],  feed_dict=d)\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "help(tf.nn.sampled_softmax_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "samp_id = word2int[\"three\"]\n",
    "\n",
    "pS = pd.Series([w_id for w_id in word_id_counts.keys()])\n",
    "neighbor_ids = pS.apply( lambda w_id: cosineDistance(net.word2vec(samp_id), net.word2vec(w_id)) ).nlargest(6).index.values\n",
    "\n",
    "print(\"Words close to {}: {}\".format(int2word[samp_id], [int2word[i] for i in neighbor_ids] ) )\n",
    "print(\"------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pS = pd.Series(np.array([1,2,3,4,5,74,7,78]))\n",
    "pS.apply(lambda x: 1/x).nsmallest(3).index.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "toy_w2v.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
