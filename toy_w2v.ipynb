{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import itertools as itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Infinite data iterator\n",
    "def iterData(filename, batch_size, jump_around=False):\n",
    "\n",
    "    # Iterator reading file in batches\n",
    "    def get_batch():\n",
    "    \n",
    "        with open(filename, \"r\") as datfile:\n",
    "\n",
    "            position, offset = 0, batch_size\n",
    "\n",
    "            while True:\n",
    "                \n",
    "                if jump_around:\n",
    "                    offset = np.random.randint(1,4) * batch_size\n",
    "                    \n",
    "                yield datfile.read(batch_size)\n",
    "                position = datfile.seek(position + offset)\n",
    "\n",
    "    batch = get_batch()\n",
    "\n",
    "    # When file is exhausted, start over with new get_batch iterator\n",
    "    while True:\n",
    "        b = next(batch)\n",
    "        if b:\n",
    "            yield  b.split()\n",
    "        else:\n",
    "            print(\"EOF, starting over\")\n",
    "            batch = get_batch()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "EOF, starting over\n",
      "['Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly', 'Warl', 'd! is', 'jell', 'y and', 'smel', 'ly an', \"d it'\", 'll do', 'otool', 'oo!', 'Jelly']\n"
     ]
    }
   ],
   "source": [
    "# Test infinite iterator\n",
    "fn = \"./testf\"\n",
    "\n",
    "with open(fn, \"r\") as tf:\n",
    "    lines = tf.read()\n",
    "print(len(lines))\n",
    "\n",
    "# Only 54 charactors reused indefinitely\n",
    "testIter = iterData(fn, 5)\n",
    "print(list( (\" \".join(next(testIter)) for k in range(100)) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Initialize file iterator\n",
    "from config import settings\n",
    "\n",
    "moreData = iterData(settings[\"data_path\"], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220824\n"
     ]
    }
   ],
   "source": [
    "# Construct vocabulary set\n",
    "words = Counter(next(moreData))\n",
    "print(len(words))\n",
    "for k in range(70000):\n",
    "    words.update(next(moreData))\n",
    "print(len(words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39115\n"
     ]
    }
   ],
   "source": [
    "# Subsampling with word2vec's subsampling function\n",
    "# following: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "\n",
    "# Increase the chance of a less common word being kept, and reduce that of very common words\n",
    "def P(word_fraction):\n",
    "\n",
    "    return np.sqrt(1e-7/word_fraction) + 1e-7/word_fraction\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "m = min(words.values())\n",
    "\n",
    "total_words = sum(words.values())\n",
    "\n",
    "words = Counter({ w : int( (count/m)**0.75 )  for w, count in words.most_common() if np.random.uniform() >= P(count/total_words)})\n",
    "\n",
    "\n",
    "# Word to id mappings\n",
    "word2int = { tup[0] : i for i, tup in enumerate(words.most_common()) }\n",
    "int2word = { i : word for word, i in word2int.items() }\n",
    "\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Skip-gram model: for each word, sample a surrounding word within a fixed window (skip-window) excluding itself,\n",
    "#  each word is processed in this way k times (skip number) generating k training targets for it. \n",
    "#\n",
    "\n",
    "def inputsTargets(word_sequence, radius = 4, repeat_num = 2):\n",
    "\n",
    "    words, targets = [None]*len(word_sequence)*repeat_num, [None]*(len)(word_sequence)*repeat_num\n",
    "    batch_size = len(words) # or targets\n",
    "    \n",
    "    for i in range(0,batch_size, repeat_num):\n",
    "        \n",
    "        # Index in word_sequence array ( len(word_sequence) < batch_size )\n",
    "        index = i // repeat_num\n",
    "        word = word_sequence[index]\n",
    "        \n",
    "        lower = 0 if index < radius else index - radius\n",
    "        upper = index + radius\n",
    "        words_in_window = word_sequence[lower:index] + word_sequence[index+1:upper]\n",
    "        \n",
    "        for k in range(repeat_num):\n",
    "            words[i+k] = word\n",
    "            targets[i+k] = words_in_window[np.random.randint(0,len(words_in_window))]\n",
    "\n",
    "    return words, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class littleNN(object):\n",
    "\n",
    "    def __init__(self, word_id_counts, embedding_dim, neg_sample_size, learning_rate = 0.05):\n",
    "\n",
    "        ## Vocabulary\n",
    "        self.vocab_size = len(word_id_counts)\n",
    "        self.unigram_table = list(word_id_counts.elements()) # immitating original w2v\n",
    "        \n",
    "        ## Network parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        \n",
    "        # layers\n",
    "        self.w0 = np.random.normal(0, self.vocab_size**-0.5, (self.vocab_size, embedding_dim ) )\n",
    "        self.b0 = np.zeros(embedding_dim)\n",
    "        \n",
    "        # self.w0 = np.zeros( (self.vocab_size, embedding_dim ) )                        \n",
    "        # self.b0 = np.random.normal(0, self.vocab_size**-0.5,  embedding_dim  )\n",
    "        \n",
    "        self.w1 = np.random.normal(0, self.vocab_size**-0.5, (embedding_dim, self.vocab_size) )\n",
    "        self.b1 = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # self.w0 = np.random.uniform(low=-init_range, high=init_range, size=(self.vocab_size, embedding_dim ) )\n",
    "        # self.w1 = np.random.uniform(low=-init_range, high=init_range, size=(embedding_dim, self.vocab_size) )\n",
    "        # self.w0 = np.zeros( (self.vocab_size, embedding_dim ) )\n",
    "        # self.w1 = np.zeros( (embedding_dim, self.vocab_size) )\n",
    "        \n",
    "        # sigmoid activation\n",
    "        self.sgmd = lambda x: 1 / ( 1 + np.exp(-x))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    # Softmax\n",
    "    def Softmax(self, v):\n",
    "\n",
    "        # Numerical stability, (avoiding large number overflow)\n",
    "        C = max(v)\n",
    "\n",
    "        expV = np.exp(v-C)\n",
    "        return expV / sum(expV)\n",
    "    \n",
    "    # Negative sampling\n",
    "    def negSample(self):\n",
    "\n",
    "        neg_samples = {}\n",
    "        for k in range(self.neg_sample_size):\n",
    "\n",
    "            # Imitating unigram table idea from original w2v, see;\n",
    "            # http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "            neg_word_id = self.unigram_table[np.random.randint(len(self.unigram_table))]\n",
    "            \n",
    "            neg_samples[neg_word_id] = 0 # one-hot encoding to 0 \n",
    "\n",
    "        return neg_samples\n",
    "\n",
    "    \n",
    "    def crossEntropy(self, softmax_ps, target_id):\n",
    "        \n",
    "        return -np.log(softmax_ps[target_id])\n",
    "    \n",
    "    # Forward pass\n",
    "    def forwardPass(self, inword_id):\n",
    "\n",
    "        # Input to hidden is just a lookup (b/c of one hot word encoding)\n",
    "        z0 = self.w0[inword_id] + self.b0\n",
    "        a0 =  z0 #self.sgmd(z0)\n",
    "\n",
    "        # Hidden to output\n",
    "        z1 = np.dot(a0, self.w1) + self.b1\n",
    "        softmax_ps = self.Softmax(z1)\n",
    "\n",
    "        return a0, softmax_ps\n",
    "\n",
    "    # Sampled back-prop\n",
    "    def sampledBackProp(self, inword_id, a0, softmax_out, target_id):\n",
    "\n",
    "        sampled_targets = self.negSample()\n",
    "        sampled_targets[target_id] = 1 # one-hot for target is 1\n",
    "\n",
    "\n",
    "        ## Sampled backprop ##\n",
    "\n",
    "        # Hidden to output calculations\n",
    "        w0_error = np.zeros(self.embedding_dim)\n",
    "        delta_weights1= {} # dictionary lookup to update only affected terms\n",
    "        \n",
    "        for kth, row in enumerate(self.w1):\n",
    "\n",
    "            sm = 0\n",
    "            for idx in sampled_targets:\n",
    "                \n",
    "                out_err_at_idx = softmax_out[idx] - sampled_targets[idx]\n",
    "\n",
    "                ###############################################\n",
    "                # 'Local' gradient for softmax node\n",
    "                if kth == idx:\n",
    "                    softmax_derivative = softmax_out[idx] * (1 - softmax_out[idx] )\n",
    "                else:\n",
    "                    softmax_derivative = - softmax_out[idx] * softmax_out[kth]\n",
    "                ###############################################\n",
    "\n",
    "                if kth == idx:\n",
    "                    delta_weights1[ (kth,idx) ] = a0[kth] * (softmax_out[idx] - 1)#* softmax_derivative * out_err_at_idx\n",
    "                else:\n",
    "                    delta_weights1[ (kth,idx) ] = 0\n",
    "                    \n",
    "                # Dot transpose of w1\n",
    "                sm += row[idx] * (softmax_out[idx] - 1) # * softmax_derivative * out_err_at_idx\n",
    "\n",
    "            w0_error[kth] = sm\n",
    "\n",
    "        ## Input to hidden calculations\n",
    "        # Note:\n",
    "        # one-hot encoded input means only kth row of w0\n",
    "        # will be updated with non-zero deltas, since input xk is coeficient.\n",
    "\n",
    "        # b/c of one-hot input this happens to be a row vector\n",
    "        delta_weights0 = w0_error * 1 #* a0 * (1 - a0) \n",
    "\n",
    "        return (inword_id, delta_weights0), delta_weights1\n",
    "    ##\n",
    "    def updateWeights(self, deltas0, deltas1):\n",
    "        '''\n",
    "        Inputs\n",
    "        ------\n",
    "        deltas0: \n",
    "        A tuple, (index, delta_vector) index of w0 row to be updated and deltas\n",
    "        \n",
    "        deltas1:\n",
    "        Dictionary, keys are tuples corresponding to entry i,j of w1 to be updated.\n",
    "        Values are the deltas at index i,j\n",
    "        \n",
    "        '''\n",
    "\n",
    "        # Update w1\n",
    "        for key in deltas1:\n",
    "            i,j = key\n",
    "\n",
    "            self.w1[i][j] -= self.lr * deltas1[key]\n",
    "\n",
    "            # update b1\n",
    "            self.b1[j] -= deltas1[key]\n",
    "            \n",
    "        # Update w0 \n",
    "        row_index, deltas0 = deltas0\n",
    "        self.w0[row_index] -= self.lr * deltas0\n",
    "        #update b0\n",
    "        self.b0 -= deltas0\n",
    "\n",
    "    ##\n",
    "    def word2vec(self, in_word_id):\n",
    "\n",
    "        return self.w0[in_word_id]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def unit(v):\n",
    "    return v / np.sqrt(np.dot(v,v))\n",
    "\n",
    "def cosineDistance(v,w):\n",
    "    return np.dot( unit(v), unit(w) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krolik/anaconda3/envs/deep/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7131185084903153e+41\n",
      "1.7131185084903153e+41\n",
      "inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-0db4157fed1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mword_id_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mword2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtrainNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_id_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-0db4157fed1e>\u001b[0m in \u001b[0;36mtrainNN\u001b[0;34m(word_id_counts)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# print (np.mean(softmax_probs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0md0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampledBackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-f32cd4fbbdc1>\u001b[0m in \u001b[0;36msampledBackProp\u001b[0;34m(self, inword_id, a0, softmax_out, target_id)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0msoftmax_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoftmax_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0msoftmax_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoftmax_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmax_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def trainNN(word_id_counts):\n",
    "\n",
    "    # Training Params\n",
    "    epochs = 30\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # Hyperparams\n",
    "    embedding_dim = 128\n",
    "    neg_sample_size = 64\n",
    "    learning_rate = 1\n",
    "    momentum = 0.5\n",
    "\n",
    "    # word2vec and validation\n",
    "    radius = 2      # sample window radius\n",
    "    repeat_num = 3  # number of times to sample each word\n",
    "    head_subset_size = 100 # sample only from head of distribution for monitoring progress\n",
    "    validation_sample_size = 6\n",
    "    \n",
    "    # Data iterator\n",
    "    moreData = iterData(settings[\"data_path\"], batch_size) # data iterator\n",
    "    \n",
    "    # Init NN\n",
    "    net = littleNN(word_id_counts, embedding_dim, neg_sample_size, learning_rate)\n",
    "\n",
    "    ###############\n",
    "    # Random sample from dictionary to observe closest cosine distances\n",
    "    head_of_distribution = [ id_count[0] for id_count in word_id_counts.most_common( head_subset_size )]\n",
    "    samp_ids = random.sample(head_of_distribution, validation_sample_size)\n",
    "    pS = pd.Series([w_id for w_id in word_id_counts.keys() if w_id not in samp_ids])\n",
    "        \n",
    "    ##############\n",
    "    #\n",
    "    initw = net.w0\n",
    "    initb = net.b0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # for v in [net.word2vec(i) for i in samp_ids]:\n",
    "        #     print(v)\n",
    "        # print(\"-------------------\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # ## View sampled words' neighbors every epoch\n",
    "        # for samp_id in samp_ids:\n",
    "        #     neighbor_ids = pS.apply( lambda w_id: cosineDistance(net.word2vec(samp_id), net.word2vec(w_id)) ).nlargest(6).index.values\n",
    "        #     print(\"Words close to {}: {}\".format(int2word[samp_id], [int2word[i] for i in neighbor_ids] ) )\n",
    "        # print(\"------------\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "        ## Train ##\n",
    "        \n",
    "        batch = next(moreData)\n",
    "        words, targets = inputsTargets(batch, radius=radius, repeat_num=repeat_num ) \n",
    "        average_cost = 0\n",
    "        \n",
    "        for nth, word in enumerate(words):\n",
    "            \n",
    "            word_id, target_id  = word2int.get(word,None), word2int.get(targets[nth], None)\n",
    "\n",
    "            if not word_id or not target_id:\n",
    "                continue\n",
    "            \n",
    "            a0, softmax_probs = net.forwardPass(word_id)\n",
    "            # print (np.mean(softmax_probs))\n",
    "            d0, d1 = net.sampledBackProp(word_id, a0, softmax_probs, target_id)\n",
    "            net.updateWeights(d0,d1)\n",
    "\n",
    "            # \n",
    "            ce = net.crossEntropy(softmax_probs, target_id)\n",
    "            if ce:\n",
    "                average_cost += ce\n",
    "\n",
    "        #\n",
    "        \n",
    "        print( sum ( sum( abs(np.abs(net.w0) - initw) ) ) ) \n",
    "        \n",
    "        print(sum(abs(abs(net.b0) - initb)))\n",
    "        \n",
    "        print(average_cost / len(words))\n",
    "\n",
    "            \n",
    "## Run training\n",
    "word_id_counts = Counter( { word2int[w] : words[w] for w in words } )\n",
    "\n",
    "trainNN(word_id_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pS = pd.Series(np.array([1,2,3,4,5,74,7,78]))\n",
    "pS.apply(lambda x: 1/x).nsmallest(3).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "testN = littleNN(word_id_counts, 300, 5, )\n",
    "\n",
    "word_id = word2int['weather']\n",
    "targ_id = word2int['wind']\n",
    "\n",
    "a0, sfm = testN.forwardPass(word_id)\n",
    "d0,d1 = testN.sampledBackProp(word_id, a0, sfm, targ_id)\n",
    "testN.updateWeights(d0,d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,    -2,    -6,   -12,   -20, -5402,   -42, -6006])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,2,3,4,5,74,7,78])\n",
    "t * (1 - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "toy_w2v.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
